<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Neural Networks | Kang&#39;s</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="Sometimes Coder;Sometimes Painter" />
  
  
  
    <meta name="baidu-site-verification" content="true" />
  
  
  <meta name="description" content="Neural networks have a large circle of acquaintance engaged in mathematics, engineering, economics and many others due to their excellent performance in function approximation, pattern recognition, as">
<meta name="keywords" content="Neural Networks">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks">
<meta property="og:url" content="http://yoursite.com/2018/11/20/Networks/index.html">
<meta property="og:site_name" content="Kang&#39;s">
<meta property="og:description" content="Neural networks have a large circle of acquaintance engaged in mathematics, engineering, economics and many others due to their excellent performance in function approximation, pattern recognition, as">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-01-18T23:06:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Networks">
<meta name="twitter:description" content="Neural networks have a large circle of acquaintance engaged in mathematics, engineering, economics and many others due to their excellent performance in function approximation, pattern recognition, as">
  
    <link rel="alternate" href="/atom.xml" title="Kang&#39;s" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/fashion.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

</head>



  <body data-spy="scroll" data-target="#toc" data-offset="50">


  
  
  <div class="site-header-image">
    <img id="originBg" width="100%" alt="Hike News" src="">
  </div>

  <div id="header-blur" class="site-header-image blur" style="position: absolute; top:0; height: 207px; min-height: 207px; min-width: 100%;">
    <img id="blurBg" width="100%" style="top: 96%" alt="Hike News" src="">
  </div>

  <script>
        var imgUrls = "css/images/pose01.jpg,https://source.unsplash.com/collection/954550/1920x1080,https://source.unsplash.com/collection/954550/1920x1081".split(",");
        var random = Math.floor((Math.random() * imgUrls.length ));
        if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
          document.getElementById("originBg").src=imgUrls[random];
          document.getElementById("blurBg").src=imgUrls[random];
        } else {
          document.getElementById("originBg").src='/' + imgUrls[random];
          document.getElementById("blurBg").src='/' + imgUrls[random];
        }
    </script>




<header id="allheader" class="site-header" role="banner" 
   style="width: 100%; position: absolute; top:0; background: rgba(255,255,255,.8);"  >
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" rel="home" >
                <img style="margin-bottom: 10px;"  width="124px" height="124px" alt="Hike News" src=" /css/images/logo.png">
              </a>
            
          </h1>
          
          
            <div class="site-description">Sometimes a Coder</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows"  style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">Tags</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">About</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Networks" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      Neural Networks
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2018/11/20/Networks/" class="article-date">
	  <time datetime="2018-11-20T04:48:25.000Z" itemprop="datePublished">November 20, 2018</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
 
      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>Neural networks have a large circle of acquaintance engaged in mathematics, engineering, economics and many others due to their excellent performance in function approximation, pattern recognition, associative memories, forecasting and generation of new meaningful patterns. Applications of neural networks in wastewater treatment systems require full understandings of these all. Therefore, for further comprehension of this methodology, this section provides a preliminary understanding of neural networks starting with their development and structure, and emphasizes more on their classification and technical coupling with other machine learning methods. </p>
<h2 id="Evolution-from-artificial-neuron-to-deep-learning"><a href="#Evolution-from-artificial-neuron-to-deep-learning" class="headerlink" title="Evolution: from artificial neuron to deep learning"></a>Evolution: from artificial neuron to deep learning</h2><p>The past six decades have witnessed ups and downs in the development of neural networks. The modern era of neural networks began with the pioneering work of McCulloch and Pitts (1943), who proposed a neurophysiology and mathematics based model that appeared to calculate any computable functions. The next major advance came in six years later that explicit statement of a physiological learning rule for synaptic modification was presented to support for the development of the following computational models. After that, the golden age prelude opened. Practical neuron-computers were developed one after another, the most classical of which were recognized as the perceptron and adaptive linear element respectively. With a booming in the research of neural networks, intelligent learning machines seemed to be created already. However, an analysis of the weakness of the perceptron when confronting with nonlinearly problems put an end to this overestimation. The enthusiasm faded slowly combined with declining research funds at the same time, however, the prolonged silence built the foundation of theories for the still continuing renaissance. Until the introduction of the back-propagation of error learning procedure did the research of neural networks whipped into its right track again. Recently, although originally viewed with scepticism, neural networks have undergone a renaissance in the form of deep learning ( such as Deep Feedforward Networks, Convolutional Neural Networks, Generative Adversarial Networks and AutoEncoder), as a result of the development of novel training rules, an expansion in the number of layers, the access of large-scale datasets and better hardware implementations. Although researches based on the principles of the neuron doctrine are far from being finished, neural networks in computer science, benefiting from their overwhelming self-adaptability, self-organization and self-learning, have stretched into every walk of life.</p>
<h2 id="Hierarchy-from-node-to-network"><a href="#Hierarchy-from-node-to-network" class="headerlink" title="Hierarchy: from node to network"></a>Hierarchy: from node to network</h2><p>Biologically, an activated neuron acts like an interchange station transferring chemical substance to the connected ones. Chemical substance converts the electric potential in neurons, and we define this neuron is activated if the electric potential exceeds a specific threshold. Similarly, the neuron in computer science is just an information-process unit derives from the biological neuron (Haykin, 1998). It consists of three basic elements:<br>i. A bunch of weighted connections referred to as w_ij and linking two neurons i and j;<br>ii. An accumulator calculating a weighted sum of input signals x_ij;<br>iii. An activation function referred to as f and judging whether or not to send its activation value in turn down to other connecting ones depending on difference between the weighted sum and a threshold value.</p>
<p>The most common activation functions are summarized in Table.2.<br>In mathematical terms, taking the threshold value in the form of bias into consideration, the output of a neuron i can be described by the following equation:<br>y_i=f(∑_1^m▒w_ij  x_ij+bias)<br>Neurons are the element of neural networks which combines neurons in a specific network topology. Generally, in the network, data are introduced to the input layer with further procession in the following layers and constitution of an overall response to the initial inputs in the output layer (Haykin, 1998; Kriesel, 2007). Neural networks consisting of three or more fully/partly connected layers (an input and an output layer with one or more hidden layers) of linearly/nonlinearly-activating nodes are described as multilayer perceptron (MLP), which is the most basic and frequently used form of neural networks. The information learned by neural networks is stored in connecting weights and bias, which are elementary parameters of neural networks. Naturally, the number of layers and the number of nodes per layer are meta-parameters. The learning process adjusting elementary parameters and validation process to optimize meta-parameters will be introduced in Section 3. </p>
<h2 id="Classification-supervised-or-unsupervised"><a href="#Classification-supervised-or-unsupervised" class="headerlink" title="Classification: supervised or unsupervised"></a>Classification: supervised or unsupervised</h2><p>Benefit from their respective topologies, different networks show distinct advantages in solving different problems. Traditionally, neural networks are classified based on the following categories : (i) topological structure involved in the information flow of networks, (ii) the degree of learning supervision, (iii) the learning algorithms. In this section, we describe several popular networks briefly in the order of learning supervision with some typical topological structure introduced in sub-classifications. Meanwhile, considering complicated issues and special requirements in wastewater treatment, hybrid frameworks, not a particular type, are added to illustrate the strong technical coupling of neural networks with other machine learning methods.</p>
<h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><p>Supervised learning means inferring a model from labeled training data (Mohri et al., 2012). Among supervised learning, feed-forward neural networks (FFNNs) and recurrent neural networks (RNNs) are representatives of two disparate topological structures. FFNNs consist of neurons organized in layers with information flowing forward, from the input layer, through the hidden layer(s) and to the output layer. Each neuron in each layer is always completely linked to those in the neighboring layer (Fig.3). Back-propagation based FFNNs, due to their efficiency, conciseness and flexibility, are the most commonly used type (Basheer and Hajmeer, 2000). Back-propagation means error is transmitted in the opposite direction against data flow. Back-propagation algorithm was created by Paul Werbos and reorganized by Rumelhart et al (Rumelhart, 1986a; Werbos, 1974). So important it is that we must introduce back-propagation in detail in Section 3 when talking about model training. Radial basis function (RBF) networks are special cases of three-layered back-propagation based networks but always listed out separately to make comparisons with the back-propagation based networks. RBF networks employ RBFs (such as Gaussian kernel, Multiquadric and Inverse quadratic) working as activation functions in the sole hidden layer to cluster inputs of the network and implement a linear combination of RBFs in the output layer (Park and Sandberg, 1993, 1991). RBF networks are trained faster than back-propagation based ones but not as versatile (Basheer and Hajmeer, 2000). Convolutional Neural Networks are a specialized kind of deep, feed-forward networks for processing data known as grid-like topology (Goodfellow et al., 2016). The shared-weights architecture and translation invariance characteristics achieved by convolutional layer and pooling layer make them tremendously successful in practical applications involved in computer vision. Generative Adversarial Networks is a combination of twins sub-networks working together: one generates content and the other judges it. The discriminating network receives either training data or generated content from the generative network. Its discriminating ability is sent to the generating network as a feedback, which creates a form of competition making the discriminator work better at distinguishing real data from generated data and the generator learn to become less predictable to the discriminator. Different from FFNNs, RNNs make output signals fed back to neurons in the same or previous layers. The information and connection feedbacks allow the current state of RNNs to depend not only on the current inputs but also on the network state in the previous time steps. Therefore, the dynamic memory of RNNs plays an important role in solving changes related to time variations. The most common ones are Elman networks, combing hidden-layered neuron’s outputs with signals from input layer next time step as inputs of neurons in the hidden layer (Williams and Zipser, 1989). Another typical RNNs, Hopfield neural networks, are two-layered networks with an energy function serving as guarantee to converge to local minimums, which makes them efficient in solving optimization problems (Sathasivam, 2008). Long/short term memory (LSTM) networks is a kind of RNNs equipped with memory cells, each of which has three gates: input, output and forget. The input gate and the output gate determine strength of the information flow from the previous layer and to the following layer, respectively. The forget gate determines how much information in the current cell to forget. With the adjustable memory, LSTMs have been proved to be able to learn complex interrelationship from sequence problems.</p>
<h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><p>Unsupervised learning means inferring a model to describe the hidden structure from unlabeled training data (Mohri et al., 2012). Networks affiliated to unsupervised learning adaptively update a certain bunch of weights related to the winning output neuron, which derives from competitions between all output-layered neurons (Basheer and Hajmeer, 2000). The most common are Self-Organizing Maps (SOMs), which are also known as Kohonen networks. In SOMs, output-layered neurons are not isolated but interconnected with the neighboring ones in the form of two or three dimensional matrix (Kalteh et al., 2008; Kohonen, 1982). It is convenient for mapping input data to a low dimensional space but the internal topological structure of high-dimensional characteristics is maintained in parallel, which provides SOMs significant advantages in clustering and data compression (Kohonen and Honkela, 2007). Adaptive resonance theory (ART) networks, another frequently-used ones, do not modify the learned information stored in the weight vectors when presented with a new pattern but enlarge memory capacity synchronously with the increase of patterns (Carpenter and Grossberg, 2003; Grossberg, 2013). AutoEncoder is a typical unsupervised learning model attached to deep learning. It is trained to copy its input to its output by learning to compress data from the input layer into a short code and then to uncompress that code into something matching the original data. This forces the AutoEncoder to engage in dimensionality reduction and learning how to ignore noise.</p>
<h3 id="Hybrid-frameworks"><a href="#Hybrid-frameworks" class="headerlink" title="Hybrid frameworks"></a>Hybrid frameworks</h3><p>Hybrid frameworks are not a particular kind of neural networks, but one combining traditional neural networks with other machine learning methods (e.g. fuzzy system, reinforcement learning, or GA) to dovetail their respective superiority. Most of the developments on hybrid GA and neural networks focus on the exploitation of an enhancement to the design of neural networks, especially in the determination of the network structure. GAs evolve network topologies, starting with a set of arbitrary parameters and ensure the global optimal of the network topology. Likewise, particle swarm optimization (PSO), another type of global and population-based algorithm, has the same capability in improving the design of neural networks as that of GAs. Reinforcement learning, a kind of data-free method, pursues the maximum reward of adjusted actions based on the relationship between agent and environment. Therefore, it is perfect for system control especially for a lack of training data to implement supervised learning methods. Fuzzy neural networks, due to the fuzzy knowledge base brought by fuzzy system, are accomplished in expressing the fuzzy rules that are required during system control. Apparently, hybrid frameworks have the capacity to deal with more complex issues but are recommended to be avoided if single networks perform well enough, because complicated and coupled model structure impose extra computational burden.</p>
<p>Indeed, there are no “one-size-fit-all” neural networks, but a relatively better choice, just like a key to a lock, does exist for a given problem. SOMs work better in classification and data visualization; an optimization problem requires Hopfield networks; back-propagation and RBF based networks may be appropriate for forecasting and controlling; RNNs are perfect for time series problems. Therefore, the selection of neural network architecture foreshadows the effective handling of the problem to be resolved.</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a></li></ul>

      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/01/06/Stack-Queue-and-linked-list/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Stack,Queue and linked list
        
      </div>
    </a>
  
  
    <a href="/2018/11/15/linux-shell-Basic/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[linux]shell Basic</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Evolution-from-artificial-neuron-to-deep-learning"><span class="nav-number">1.</span> <span class="nav-text">Evolution: from artificial neuron to deep learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hierarchy-from-node-to-network"><span class="nav-number">2.</span> <span class="nav-text">Hierarchy: from node to network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification-supervised-or-unsupervised"><span class="nav-number">3.</span> <span class="nav-text">Classification: supervised or unsupervised</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Supervised-Learning"><span class="nav-number">3.1.</span> <span class="nav-text">Supervised Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised-Learning"><span class="nav-number">3.2.</span> <span class="nav-text">Unsupervised Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hybrid-frameworks"><span class="nav-number">3.3.</span> <span class="nav-text">Hybrid frameworks</span></a></li></ol></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 Kang&#39;s All Rights Reserved.
        
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hipaper" target="_blank">hipaper</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
      var headerblur = document.getElementById("header-blur");
      headerblur.style.minHeight = window.getComputedStyle(document.getElementById("allheader"), null).height;
    
    
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
